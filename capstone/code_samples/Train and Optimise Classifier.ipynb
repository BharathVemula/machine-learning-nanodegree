{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\n",
      "Training and optimisation report with 50 principal component\n",
      "\n",
      "KNN with 5 neighbours\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [60199, 9801]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c7c25e63fd69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mprint\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m#             print accuracy, (c, degree, gamma, kernel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tmartin/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tmartin/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tmartin/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [60199, 9801]"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "\n",
    "# with open('train.txt', 'w') as f:\n",
    "#     f.write(cap.stdout)\n",
    "\n",
    "# del cap \n",
    "\n",
    "from itertools import product\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# components = [ 25, 50, 100 ]\n",
    "components = [ 50 ]\n",
    "y = pickle.load(open('target.pkl', 'r'))\n",
    "\n",
    "best_accuracy_overall = 0\n",
    "best_params_overall = None\n",
    "y_pred_best = None\n",
    "\n",
    "for c in components:\n",
    "    \n",
    "    print '-------------------------------------------------------'\n",
    "    print\n",
    "    print 'Training and optimisation report with {0} principal component'.format(c)\n",
    "    print\n",
    "    filename = 'data-{0}-components.pkl'.format(c)\n",
    "    X = pickle.load(open(filename, 'r'))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.14, random_state=0)\n",
    "\n",
    "    y_pred = np.empty(len(y_test), dtype=np.int)\n",
    "    \n",
    "    #n_neighbors_param = [2, 3, 4, 5]\n",
    "    n_neighbors_param = [5]\n",
    "    \n",
    "    for n in n_neighbors_param:\n",
    "        \n",
    "        print 'KNN with {0} neighbours'.format(n)\n",
    "        \n",
    "        knn_clf = KNeighborsClassifier(n_neighbors=int('{}'.format(n)))\n",
    "        knn_clf.fit(X_train, y_train)\n",
    "        \n",
    "        kneighbors = knn_clf.kneighbors(X_test, return_distance=False)\n",
    "        \n",
    "        best_accuracy = 0\n",
    "        best_params = []\n",
    "        \n",
    "        #c_param = [1,10,100,1000]\n",
    "        c_param = [1]        \n",
    "        #degree_param = [3,4,5,6]\n",
    "        degree_param = [3]\n",
    "        #gamma_param = [1e-3, 1e-4]\n",
    "        gamma_param = [1e-3]\n",
    "        #kernel_param = ['poly', 'rbf']\n",
    "        kernel_param = ['poly']\n",
    "        for params in product(c_param, degree_param, gamma_param, kernel_param):\n",
    "            \n",
    "            c = params[0]\n",
    "            degree = params[1]\n",
    "            gamma = params[2]\n",
    "            kernel = params[3]\n",
    "        \n",
    "            for idx, indices in enumerate(kneighbors):\n",
    "                neighbors = [X_train[i] for i in indices]\n",
    "                neighbors_labels = [y_train[i] for i in indices]\n",
    "\n",
    "                if len(set(neighbors_labels)) == 1:\n",
    "                    y_pred[idx] = neighbors_labels[0]\n",
    "                else:\n",
    "                    svm_clf = svm.SVC(kernel='{}'.format(kernel), degree=int('{}'.format(degree)), gamma=float('{}'.format(gamma)), C=int('{}'.format(c)), random_state=0)\n",
    "                    svm_clf.fit(neighbors, neighbors_labels)\n",
    "                    label = svm_clf.predict(X_test[idx].reshape(1, -1))\n",
    "                    y_pred[idx] = label\n",
    "            \n",
    "#             print accuracy_score(y_train, y_pred)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "#             print accuracy, (c, degree, gamma, kernel)\n",
    "            if accuracy >= best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params.append((c, degree, gamma, kernel))\n",
    "                \n",
    "                if best_accuracy >= best_accuracy_overall:\n",
    "                    \n",
    "                    clf_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "                    \n",
    "                    print clf_confusion_matrix\n",
    "                    \n",
    "                    def plot_confusion_matrix(cm, classes, title, cmap=plt.cm.Reds):\n",
    "                        \"\"\"\n",
    "                        This function prints and plots the confusion matrix.\n",
    "                        Normalization can be applied by setting `normalize=True`.\n",
    "                        \"\"\"\n",
    "                        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "                        plt.title(title)\n",
    "                        plt.colorbar()\n",
    "                        tick_marks = np.arange(len(classes))\n",
    "                        plt.xticks(tick_marks, classes)\n",
    "                        plt.yticks(tick_marks, classes)\n",
    "\n",
    "                        thresh = cm.max() / 2.\n",
    "                        for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "                            plt.text(j, i, cm[i, j],\n",
    "                                     horizontalalignment=\"center\",\n",
    "                                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "                        plt.tight_layout()\n",
    "                        plt.ylabel('True label')\n",
    "                        plt.xlabel('Predicted label')\n",
    "\n",
    "                    # Compute confusion matrix\n",
    "                    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                    np.set_printoptions(precision=2)\n",
    "\n",
    "                    # Plot non-normalized confusion matrix\n",
    "                    plt.figure()\n",
    "                    plot_confusion_matrix(cnf_matrix, classes=[0,1,2,3,4,5,6,7,8,9], title='Predicted vs True label')\n",
    "\n",
    "                    plt.show()\n",
    "                    \n",
    "                    \n",
    "                    error_per_digit = []\n",
    "                    label = 0 \n",
    "                    for row in clf_confusion_matrix:\n",
    "                        num = float(row[label])\n",
    "                        den = sum(row)\n",
    "                        error = 1 - (num / den)\n",
    "                        error_per_digit.append(error)\n",
    "                        label += 1\n",
    "\n",
    "                    for e in error_per_digit:\n",
    "                        print e\n",
    "                    \n",
    "                    best_accuracy_overall = best_accuracy\n",
    "                    best_params_overall = (n, c, degree, gamma, kernel)\n",
    "                    y_pred_best = y_pred\n",
    "        \n",
    "        print 'accuracy:', best_accuracy\n",
    "        print 'paramters:', best_params\n",
    "        print\n",
    "    \n",
    "    print '-------------------------------------------------------'\n",
    "\n",
    "print 'best_accuracy_overall:', best_accuracy_overall\n",
    "print 'paramters:', best_params_overall\n",
    "print accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "params_best_file = 'params_best.pkl'\n",
    "file = open(params_best_file, 'w')\n",
    "pickle.dump(params_best, file)\n",
    "file.close()\n",
    "    \n",
    "y_pred_best_file = 'y_pred_best.pkl'\n",
    "file = open(y_pred_best_file, 'w')\n",
    "pickle.dump(y_pred_best, file)\n",
    "file.close()\n",
    "\n",
    "y_test_file = 'y_test_best.pkl'\n",
    "file = open(y_test_file, 'w')\n",
    "pickle.dump(y_test, file)\n",
    "file.close()\n",
    "     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
